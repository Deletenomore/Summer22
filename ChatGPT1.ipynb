{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/2YApnq6NB5QV+litEdaw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/everestso/Summer22/blob/main/ChatGPT1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install python-dotenv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJODhKhVkRqd",
        "outputId": "491c4227-1186-4f3f-df02-b803efc40101"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9v_igqKotPwx",
        "outputId": "cd26a678-4e1c-4449-e3a9-bf84cb647f17"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/\n",
        "f = open(\"ChatGptApi.txt\")\n",
        "s = f.read()\n",
        "f.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnQcJJOBvYkp",
        "outputId": "2ecd316b-6de9-4501-c35a-6218b91c8010"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uvYVnu0qkLpF"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "import os\n",
        "import textwrap\n",
        "\n",
        "openai.api_key  =  s"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=0, # this is the degree of randomness of the model's output\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "0peLUIYDl9ok"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt=f\"\"\"\n",
        "Describe in 100 words the charismatic movement within\n",
        "catholic church including charasmatic mass. Further provide potential\n",
        "for promoting growth in Europe.\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print('\\n'.join(textwrap.wrap(response,70)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7EPYng8qMYZ_",
        "outputId": "ee51f356-2c0e-40f0-bb62-cccf8c914b92"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The charismatic movement within the Catholic Church is a vibrant and\n",
            "dynamic expression of faith that emphasizes the presence and power of\n",
            "the Holy Spirit in the lives of believers. It emerged in the late\n",
            "1960s and early 1970s, drawing inspiration from Pentecostalism and\n",
            "emphasizing spiritual gifts such as speaking in tongues, healing, and\n",
            "prophecy.  One of the key aspects of the charismatic movement is the\n",
            "charismatic mass, which incorporates lively worship, spontaneous\n",
            "prayer, and a focus on personal encounter with God. Charismatic masses\n",
            "often feature uplifting music, enthusiastic participation, and a sense\n",
            "of joyful celebration.  In terms of promoting growth in Europe, the\n",
            "charismatic movement has the potential to attract and engage younger\n",
            "generations who seek a more experiential and emotionally connected\n",
            "form of worship. Its emphasis on personal encounter with God and the\n",
            "Holy Spirit can provide a refreshing and transformative experience for\n",
            "individuals seeking a deeper spiritual connection. Additionally, the\n",
            "charismatic movement's focus on community and fellowship can help\n",
            "create a sense of belonging and support, which is particularly\n",
            "important in an increasingly secular and individualistic society. By\n",
            "embracing the charismatic movement, the Catholic Church in Europe can\n",
            "tap into a powerful source of spiritual renewal and potentially\n",
            "attract new members.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " Corey Lynch, Kamelia Aryafar, and Josh Attenberg. 2016. Images Don't Lie:\n",
        " Transferring Deep Visual Semantic Features to Large-Scale Multimodal Learning to Rank.\n",
        " In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD '16).\n",
        " Association for Computing Machinery, New York, NY, USA, 541–548.\n",
        " https://doi.org/10.1145/2939672.2939728\n",
        "\"\"\"\n",
        "\n",
        "text = f\"\"\"\n",
        "Search is at the heart of modern e-commerce. As a result, the task  \\\n",
        "of ranking search results automatically (learning to rank) is a multibillion  \\\n",
        "dollar machine learning problem. Traditional models optimize  \\\n",
        "over a few hand-constructed features based on the item’s text.  \\\n",
        "In this paper, we introduce a multimodal learning to rank model that  \\\n",
        "combines these traditional features with visual semantic features  \\\n",
        "transferred from a deep convolutional neural network. In a large  \\\n",
        "scale experiment using data from the online marketplace Etsy 1,  \\\n",
        "we verify that moving to a multimodal representation significantly  \\\n",
        "improves ranking quality. We show how image features can capture  \\\n",
        "fine-grained style information not available in a text-only representation.  \\\n",
        "In addition, we show concrete examples of how image  \\\n",
        "information can successfully disentangle pairs of highly different  \\\n",
        "items that are ranked similarly by a text-only model.  \\\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Summarize the text delimited by triple backticks \\\n",
        "into a single sentence.\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"\\n\".join(textwrap.wrap(response,70)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkupPHwMmAjz",
        "outputId": "39a383d8-036f-4c9a-a704-8078a81eac29"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The text discusses the importance of search in e-commerce and the use\n",
            "of a multimodal learning to rank model that combines traditional\n",
            "features with visual semantic features to improve ranking quality in a\n",
            "large-scale experiment using data from Etsy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.austinchronicle.com/events/film/2022-12-23/whitney-houston-i-wanna-dance-with-somebody/\n",
        "# ★½  (-4)\n",
        "text = f\"\"\"\n",
        "Whitney Houston was an icon, a legend of the music industry and a breathtaking pop idol who cranked out hit after hit, shattering records that previously the Beatles and Elvis held. Houston had enormous range and depth, yet the film about this larger-than-life superstar is somehow one of the flattest, cheapest, and most underwhelming music biopics to ever land on the silver screen.  \\\n",
        "Directed by Kasi Lemmons (Harriet, Eve’s Bayou), Whitney Houston: I Wanna Dance With Somebody somehow manages to make the life of Houston so painfully dull and thin. Just as the heat begins to rise, the film skips ahead, or around, orbiting the chance to dig deeper, to offer a more meaningful glimpse into the pop star’s history. Anthony McCarten’s script coasts, breezing through Houston’s life with a coolness that is anticlimactic. Father issues, her divorce, queer identity struggles, and addiction are danced around, never really creating a deep impact on an emotional level. When the bombastic moments like Houston’s historic performances in a newly post-apartheid South Africa arrive, they do so with a whimper, with tacky CGI stadiums that bring a tepid energy to the watered-down performances.  \\\n",
        "There’s no bite to Naomi Ackie’s performance, and perhaps that is because she’s not given anything worth chewing on. Even the clothes they drape her in feel cheap and poorly made. One of the film’s greatest injustices is that, after Ackie’s “big” performance of the iconic “I Have Nothing” medley Houston sang at the 1994 American Music Awards, the live broadcast is immediately shown in the credits, highlighting the stark difference in quality. The movie replaces her lush velvet caped black dress with a fast-fashion, wrinkled gown that’s bejeweled with clunky plastic gems. This fizzled ending echoes the sentiment of the entire film, a watered-down TV-soap approach to its dramatic subject.  \\\n",
        "Whitney Houston: I Wanna Dance With Somebody is like the SparkNotes of her life, a smattering of collected moments that feel hollow. The film retroactively makes Baz Luhrmann’s Elvis look like a masterpiece for actually trying to be bedazzling and insane, because Whitney Houston: I Wanna Dance With Somebody is so stale it might as well have been shoved directly onto a streaming platform to wither away forgotten – unlike Houston’s discography, which will be remembered for decades to come.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Evaluate the sentiment of the following quote between \"```\" \\\n",
        "from negative 10 to positive 10 and output for each just the numeric value, \\\n",
        "do not include any additional text, just the numeric value:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(\"\\n\".join(textwrap.wrap(response,70)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pntYAHByTSP",
        "outputId": "dbb1c8e6-b8f9-4055-f1c6-387ee94ded26"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.smh.com.au/culture/movies/all-the-boxing-day-movies-reviewed-what-should-you-see-or-skip-20221220-p5c7t1.html\n",
        "# Whitney Houston: I Wanna Dance with Somebody ★★★½  (+4)\n",
        "text = f\"\"\"\n",
        "What more is there to say about Whitney Houston, whose brilliant career, drug-taking and premature death has been hashed over exhaustively on screen and in the tabloids?  \\\n",
        "I Wanna Dance with Somebody tackles that question by taking her story back to the music. It’s written by New Zealander Anthony McCarten, whose screenplay for Bohemian Rhapsody didn’t dilute the tragedies in Freddie Mercury’s short life while still managing to evoke the exhilaration to be had from Queen’s greatest concerts.  \\\n",
        "It took a delicate touch and other rock stars haven’t been as fortunate. While Rocketman was just as exuberant in doing justice to Elton John as a performer, the Aretha Franklin biopic Respect (2021) became more depressing as it went on. The United States vs. Billie Holiday, also released last year, was the same. It was powerful but it was no celebration. Then there was Baz Luhrmann’s Elvis, which was more glitz than heart, clouded by the decision to tell the tale from the venal viewpoint of Presley’s manager, Colonel Tom Parker.  \\\n",
        "Members of Houston’s family were interviewed at length for a documentary four years ago. But for this film, also produced with the family’s co-operation, director Kasi Lemmons made extensive use of the singer’s original recordings, including those laid down en route to the final versions. It allowed star Naomi Ackie to take Houston’s false starts, double takes and spontaneous comments as prompts in shaping her performance. Although she was filmed singing, her voice was replaced by Houston’s.  \\\n",
        "She does well, catching the contradictions in Houston’s character – the naivete, the fear of failure, the rebellious streak and its effect on her religious convictions. But physically, Ackie bears only a superficial resemblance to the singer. With Houston’s image still so fresh in the memory, this is a distraction.  \\\n",
        "The script is impressionistic in style, covering a lot of ground by being selective and using Houston’s songs to drive the narrative. Much is made of her relationship with the record company chief Clive Davis (one of the film’s producers), who discovered Houston and helped her choose songs for her albums. He’s played by a wryly urbane Stanley Tucci, giving us a character whose iron fist is well covered in velvet. Appalled when Houston takes up smoking, he looks sadly at her as she lights up, remarking that it’s like watching a Stradivarius being left out in the rain.  \\\n",
        "The film falls a little short of its claim to be warts-and-all, condensing the low points in Houston’s turbulent marriage to Bobby Brown (Ashton Sanders), but it doesn’t hold back in dealing with her father’s financial mismanagement. Nor does it underplay the importance of her enduring friendship with her former lover, Robyn Crawford – played with a lot of verve by Nafessa Williams.  \\\n",
        "And while it leaves her death off-screen, preferring to go out with the show-stopping medley she delivered at the 1994 American Music Awards, I’m not complaining. Sandra Hall\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Evaluate the sentiment of the following quote between \"```\" \\\n",
        "from negative 10 to positive 10 and output for each just the numeric value, \\\n",
        "do not include any additional text, just the numeric value:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "response = get_completion(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "kz51EdyezVp7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86b8981e-37fc-40ab-d57a-568777c5cadd"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "January 1, 1863\n",
        "\n",
        "A Transcription\n",
        "\n",
        "By the President of the United States of America:\n",
        "\n",
        "A Proclamation.\n",
        "\n",
        "Whereas, on the twenty-second day of September, in the year of our Lord one thousand eight hundred and sixty-two, a proclamation was issued by the President of the United States, containing, among other things, the following, to wit:\n",
        "\n",
        "\"That on the first day of January, in the year of our Lord one thousand eight hundred and sixty-three, all persons held as slaves within any State or designated part of a State, the people whereof shall then be in rebellion against the United States, shall be then, thenceforward, and forever free; and the Executive Government of the United States, including the military and naval authority thereof, will recognize and maintain the freedom of such persons, and will do no act or acts to repress such persons, or any of them, in any efforts they may make for their actual freedom.\n",
        "\n",
        "\"That the Executive will, on the first day of January aforesaid, by proclamation, designate the States and parts of States, if any, in which the people thereof, respectively, shall then be in rebellion against the United States; and the fact that any State, or the people thereof, shall on that day be, in good faith, represented in the Congress of the United States by members chosen thereto at elections wherein a majority of the qualified voters of such State shall have participated, shall, in the absence of strong countervailing testimony, be deemed conclusive evidence that such State, and the people thereof, are not then in rebellion against the United States.\"\n",
        "\n",
        "Now, therefore I, Abraham Lincoln, President of the United States, by virtue of the power in me vested as Commander-in-Chief, of the Army and Navy of the United States in time of actual armed rebellion against the authority and government of the United States, and as a fit and necessary war measure for suppressing said rebellion, do, on this first day of January, in the year of our Lord one thousand eight hundred and sixty-three, and in accordance with my purpose so to do publicly proclaimed for the full period of one hundred days, from the day first above mentioned, order and designate as the States and parts of States wherein the people thereof respectively, are this day in rebellion against the United States, the following, to wit:\n",
        "\n",
        "Arkansas, Texas, Louisiana, (except the Parishes of St. Bernard, Plaquemines, Jefferson, St. John, St. Charles, St. James Ascension, Assumption, Terrebonne, Lafourche, St. Mary, St. Martin, and Orleans, including the City of New Orleans) Mississippi, Alabama, Florida, Georgia, South Carolina, North Carolina, and Virginia, (except the forty-eight counties designated as West Virginia, and also the counties of Berkley, Accomac, Northampton, Elizabeth City, York, Princess Ann, and Norfolk, including the cities of Norfolk and Portsmouth[)], and which excepted parts, are for the present, left precisely as if this proclamation were not issued.\n",
        "\n",
        "And by virtue of the power, and for the purpose aforesaid, I do order and declare that all persons held as slaves within said designated States, and parts of States, are, and henceforward shall be free; and that the Executive government of the United States, including the military and naval authorities thereof, will recognize and maintain the freedom of said persons.\n",
        "\n",
        "And I hereby enjoin upon the people so declared to be free to abstain from all violence, unless in necessary self-defence; and I recommend to them that, in all cases when allowed, they labor faithfully for reasonable wages.\n",
        "\n",
        "And I further declare and make known, that such persons of suitable condition, will be received into the armed service of the United States to garrison forts, positions, stations, and other places, and to man vessels of all sorts in said service.\n",
        "\n",
        "And upon this act, sincerely believed to be an act of justice, warranted by the Constitution, upon military necessity, I invoke the considerate judgment of mankind, and the gracious favor of Almighty God.\n",
        "\n",
        "In witness whereof, I have hereunto set my hand and caused the seal of the United States to be affixed.\n",
        "\n",
        "Done at the City of Washington, this first day of January, in the year of our Lord one thousand eight hundred and sixty three, and of the Independence of the United States of America the eighty-seventh.\n",
        "\n",
        "By the President: ABRAHAM LINCOLN\n",
        "WILLIAM H. SEWARD, Secretary of State.\n",
        "\n",
        "\"\"\"\n",
        "print(\"\\n\".join(textwrap.wrap(text,70)))"
      ],
      "metadata": {
        "id": "aKSmpGH-Ah0E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "467a92c5-94b5-4436-e0e2-7eb292e98267"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " January 1, 1863  A Transcription  By the President of the United\n",
            "States of America:  A Proclamation.  Whereas, on the twenty-second day\n",
            "of September, in the year of our Lord one thousand eight hundred and\n",
            "sixty-two, a proclamation was issued by the President of the United\n",
            "States, containing, among other things, the following, to wit:  \"That\n",
            "on the first day of January, in the year of our Lord one thousand\n",
            "eight hundred and sixty-three, all persons held as slaves within any\n",
            "State or designated part of a State, the people whereof shall then be\n",
            "in rebellion against the United States, shall be then, thenceforward,\n",
            "and forever free; and the Executive Government of the United States,\n",
            "including the military and naval authority thereof, will recognize and\n",
            "maintain the freedom of such persons, and will do no act or acts to\n",
            "repress such persons, or any of them, in any efforts they may make for\n",
            "their actual freedom.  \"That the Executive will, on the first day of\n",
            "January aforesaid, by proclamation, designate the States and parts of\n",
            "States, if any, in which the people thereof, respectively, shall then\n",
            "be in rebellion against the United States; and the fact that any\n",
            "State, or the people thereof, shall on that day be, in good faith,\n",
            "represented in the Congress of the United States by members chosen\n",
            "thereto at elections wherein a majority of the qualified voters of\n",
            "such State shall have participated, shall, in the absence of strong\n",
            "countervailing testimony, be deemed conclusive evidence that such\n",
            "State, and the people thereof, are not then in rebellion against the\n",
            "United States.\"  Now, therefore I, Abraham Lincoln, President of the\n",
            "United States, by virtue of the power in me vested as Commander-in-\n",
            "Chief, of the Army and Navy of the United States in time of actual\n",
            "armed rebellion against the authority and government of the United\n",
            "States, and as a fit and necessary war measure for suppressing said\n",
            "rebellion, do, on this first day of January, in the year of our Lord\n",
            "one thousand eight hundred and sixty-three, and in accordance with my\n",
            "purpose so to do publicly proclaimed for the full period of one\n",
            "hundred days, from the day first above mentioned, order and designate\n",
            "as the States and parts of States wherein the people thereof\n",
            "respectively, are this day in rebellion against the United States, the\n",
            "following, to wit:  Arkansas, Texas, Louisiana, (except the Parishes\n",
            "of St. Bernard, Plaquemines, Jefferson, St. John, St. Charles, St.\n",
            "James Ascension, Assumption, Terrebonne, Lafourche, St. Mary, St.\n",
            "Martin, and Orleans, including the City of New Orleans) Mississippi,\n",
            "Alabama, Florida, Georgia, South Carolina, North Carolina, and\n",
            "Virginia, (except the forty-eight counties designated as West\n",
            "Virginia, and also the counties of Berkley, Accomac, Northampton,\n",
            "Elizabeth City, York, Princess Ann, and Norfolk, including the cities\n",
            "of Norfolk and Portsmouth[)], and which excepted parts, are for the\n",
            "present, left precisely as if this proclamation were not issued.  And\n",
            "by virtue of the power, and for the purpose aforesaid, I do order and\n",
            "declare that all persons held as slaves within said designated States,\n",
            "and parts of States, are, and henceforward shall be free; and that the\n",
            "Executive government of the United States, including the military and\n",
            "naval authorities thereof, will recognize and maintain the freedom of\n",
            "said persons.  And I hereby enjoin upon the people so declared to be\n",
            "free to abstain from all violence, unless in necessary self-defence;\n",
            "and I recommend to them that, in all cases when allowed, they labor\n",
            "faithfully for reasonable wages.  And I further declare and make\n",
            "known, that such persons of suitable condition, will be received into\n",
            "the armed service of the United States to garrison forts, positions,\n",
            "stations, and other places, and to man vessels of all sorts in said\n",
            "service.  And upon this act, sincerely believed to be an act of\n",
            "justice, warranted by the Constitution, upon military necessity, I\n",
            "invoke the considerate judgment of mankind, and the gracious favor of\n",
            "Almighty God.  In witness whereof, I have hereunto set my hand and\n",
            "caused the seal of the United States to be affixed.  Done at the City\n",
            "of Washington, this first day of January, in the year of our Lord one\n",
            "thousand eight hundred and sixty three, and of the Independence of the\n",
            "United States of America the eighty-seventh.  By the President:\n",
            "ABRAHAM LINCOLN WILLIAM H. SEWARD, Secretary of State.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize in 30 words the following book between \"```\" \\\n",
        ":\n",
        "```{text}```\n",
        "\"\"\"\n",
        "\n",
        "response = get_completion(prompt)\n",
        "print(\"\\n\".join(textwrap.wrap(response,70)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8lkn34l9b63",
        "outputId": "92da256f-d0de-4d97-d531-99624c17b5da"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This book is a transcription of Abraham Lincoln's proclamation on\n",
            "January 1, 1863, declaring that all slaves in rebellious states are to\n",
            "be freed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "Pablo Picasso: Modern Artist, Master Innovator\n",
        "Pablo Picasso (1881–1973), the Spanish-born, Paris-based painter, sculptor, draftsman, printmaker, decorative artist, and writer, influenced the course of 20th-century art with almost unmatched magnitude. His passionate and often provocative life, his unfettered embrace of experimentation, and his drive for re-invention fed into his prolific production of works, upending notions of what art was supposed to look like.\n",
        "\n",
        "Picasso’s numerous inspirations ranged from history, politics, and current events to the work of fellow artists, to the world outside of his studio. In his dynamic body of work, such opposites as intellect and emotions, forms of classicism and expressionism, and the conscious and the unconscious simultaneously clashed and coalesced.\n",
        "\n",
        "He was known for his fascination with so-called “Primitive” art, a term Europeans used when referring to African masks and statuary, which for Picasso also encompassed ancient carvings from the Iberian Peninsula, the landmass that eventually would be divided into present-day Spain, Andorra, and Portugal. The blocky, pared-down forms and forceful, angular planes of this art ignited the artist’s imagination. Its striking shapes and contours made their way into his own compositions and contributed to his radical restructuring of the formal characteristics and visual impact of the work of art. Together with fellow artist Georges Braque, Picasso pioneered Cubism, a visual language of geometric planes and compressed space that splintered subjects—like the human figure, a landscape, or a still life scene—into multifaceted pieces, causing them to appear partially abstracted, flattened, and fragmented, as if reflected in a shattered mirror.\n",
        "\n",
        "Picasso’s influence stretched well beyond Cubism. Over the course of his career, he produced works that significantly shaped Surrealism and Expressionism, not to mention the ongoing resonance of his legacy still felt by artists working today.\n",
        "\n",
        "Cubism\n",
        "Following their 1907 meeting in Paris, artists Pablo Picasso and Georges Braque pioneered the Cubist style, a new vision for a new century that inspired paintings that were initially ridiculed by critics for consisting of “little cubes.” Often painting side-by-side in their Montmartre, Paris, studios, the artists developed a visual language of geometric planes and compressed space that rejected the conventions of perspective and representation. Cubist works challenged viewers to understand a subject broken down into its geometrical components and often represented from several angles at once. Traditional subjects like nude figures, landscapes, and still lifes were reinvented as increasingly fragmented compositions by Picasso, Braque, and other artists working in and around the French capital.\n",
        "\n",
        "Cubists abstracted from real life to make their work, but most often maintained small identifiable clues to a realistic figure, whether a woman or a violin. The artists adopted a neutral palette of browns and blacks, intending the viewer to focus on the geometric composition rather than the color. Cubism marks a pioneering moment in the history of art—one that ended when many of its leading practitioners, Braque among them, enlisted to fight in World War I.\n",
        "\"\"\"\n",
        "print(\"\\n\".join(textwrap.wrap(text,70)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C27tIfHxSfKy",
        "outputId": "46dc710e-929e-4305-cd06-23358cf6b1d3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Pablo Picasso: Modern Artist, Master Innovator Pablo Picasso\n",
            "(1881–1973), the Spanish-born, Paris-based painter, sculptor,\n",
            "draftsman, printmaker, decorative artist, and writer, influenced the\n",
            "course of 20th-century art with almost unmatched magnitude. His\n",
            "passionate and often provocative life, his unfettered embrace of\n",
            "experimentation, and his drive for re-invention fed into his prolific\n",
            "production of works, upending notions of what art was supposed to look\n",
            "like.  Picasso’s numerous inspirations ranged from history, politics,\n",
            "and current events to the work of fellow artists, to the world outside\n",
            "of his studio. In his dynamic body of work, such opposites as\n",
            "intellect and emotions, forms of classicism and expressionism, and the\n",
            "conscious and the unconscious simultaneously clashed and coalesced.\n",
            "He was known for his fascination with so-called “Primitive” art, a\n",
            "term Europeans used when referring to African masks and statuary,\n",
            "which for Picasso also encompassed ancient carvings from the Iberian\n",
            "Peninsula, the landmass that eventually would be divided into present-\n",
            "day Spain, Andorra, and Portugal. The blocky, pared-down forms and\n",
            "forceful, angular planes of this art ignited the artist’s imagination.\n",
            "Its striking shapes and contours made their way into his own\n",
            "compositions and contributed to his radical restructuring of the\n",
            "formal characteristics and visual impact of the work of art. Together\n",
            "with fellow artist Georges Braque, Picasso pioneered Cubism, a visual\n",
            "language of geometric planes and compressed space that splintered\n",
            "subjects—like the human figure, a landscape, or a still life\n",
            "scene—into multifaceted pieces, causing them to appear partially\n",
            "abstracted, flattened, and fragmented, as if reflected in a shattered\n",
            "mirror.  Picasso’s influence stretched well beyond Cubism. Over the\n",
            "course of his career, he produced works that significantly shaped\n",
            "Surrealism and Expressionism, not to mention the ongoing resonance of\n",
            "his legacy still felt by artists working today.  Cubism Following\n",
            "their 1907 meeting in Paris, artists Pablo Picasso and Georges Braque\n",
            "pioneered the Cubist style, a new vision for a new century that\n",
            "inspired paintings that were initially ridiculed by critics for\n",
            "consisting of “little cubes.” Often painting side-by-side in their\n",
            "Montmartre, Paris, studios, the artists developed a visual language of\n",
            "geometric planes and compressed space that rejected the conventions of\n",
            "perspective and representation. Cubist works challenged viewers to\n",
            "understand a subject broken down into its geometrical components and\n",
            "often represented from several angles at once. Traditional subjects\n",
            "like nude figures, landscapes, and still lifes were reinvented as\n",
            "increasingly fragmented compositions by Picasso, Braque, and other\n",
            "artists working in and around the French capital.  Cubists abstracted\n",
            "from real life to make their work, but most often maintained small\n",
            "identifiable clues to a realistic figure, whether a woman or a violin.\n",
            "The artists adopted a neutral palette of browns and blacks, intending\n",
            "the viewer to focus on the geometric composition rather than the\n",
            "color. Cubism marks a pioneering moment in the history of art—one that\n",
            "ended when many of its leading practitioners, Braque among them,\n",
            "enlisted to fight in World War I.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question1 = f\"What art movement did Pablo Picasso help pioneer?\"\n",
        "question2 = f\"Describe the visual language of Cubism. Support your description with details from the text.\"\n",
        "question3 = f\"What is the main idea of this text?\"\n",
        "question4 = f\"How did Picasso challenge ideas about 'what art was supposed to look like'? Give at least two details from the text to support your answer.\"\n",
        "prompt = f\"\"\"\n",
        "Given the following passage between \"```\" \\\n",
        ":\n",
        "```{text}```\n",
        "\n",
        "answer the following questions between \"```\" \\\n",
        "separate answers to each question with '' \\\n",
        "and add '\\n' to keep lines shorter than 40 characters.\n",
        "'''{question1}'''\n",
        "'''{question2}'''\n",
        "'''{question3}'''\n",
        "'''{question4}'''\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "response = get_completion(prompt)"
      ],
      "metadata": {
        "id": "uDWwrWAuSmIp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer1, answer2, answer3, answer4 =response.split('\\n')\n",
        "print (f\"\\nQ1: {question1}\")\n",
        "print(\"\\n\".join(textwrap.wrap(answer1,70)))\n",
        "print (f\"\\nQ2: {question2}\")\n",
        "print(\"\\n\".join(textwrap.wrap(answer2,70)))\n",
        "print (f\"\\nQ3: {question3}\")\n",
        "print(\"\\n\".join(textwrap.wrap(answer3,70)))\n",
        "print (f\"\\nQ4: {question4}\")\n",
        "print(\"\\n\".join(textwrap.wrap(answer4,70)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "GLO4cZ3HVRnl",
        "outputId": "48d3e184-3365-4890-d8bc-e91a5a06ca13"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-e72ee2b599e6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manswer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer4\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nQ1: {question1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nQ2: {question2}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtextwrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/GPT-4\"\n",
        "response = requests.get(url)\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# find all the text on the page\n",
        "text = soup.get_text()\n",
        "\n",
        "# find the content div\n",
        "content_div = soup.find('div', {'class': 'mw-parser-output'})\n",
        "\n",
        "# remove unwanted elements from div\n",
        "unwanted_tags = ['sup', 'span', 'table', 'ul', 'ol']\n",
        "for tag in unwanted_tags:\n",
        "    for match in content_div.findAll(tag):\n",
        "        match.extract()\n",
        "\n",
        "print(content_div.get_text())"
      ],
      "metadata": {
        "id": "iD22wlMkUFNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "article_text = content_div.get_text()\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size = 100,\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "\n",
        "texts = text_splitter.create_documents([article_text])\n",
        "print(texts[0])\n",
        "print(texts[1])"
      ],
      "metadata": {
        "id": "djYPSqb4Uqvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(texts[0])\n",
        "\n",
        "\n",
        "embedding = openai.Embedding.create(\n",
        "    input=texts[0].page_content, model=\"text-embedding-ada-002\"\n",
        ")[\"data\"][0][\"embedding\"]\n",
        "\n",
        "\n",
        "len(embedding)"
      ],
      "metadata": {
        "id": "y_qmwM38TMM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metamorphosis by Franz Kafka\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.gutenberg.org/files/5200/5200-0.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "print (response)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "# find all the text on the page\n",
        "#text = soup.get_text()\n"
      ],
      "metadata": {
        "id": "CLHCgCOrkkkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coursera AWS Generative AI Course"
      ],
      "metadata": {
        "id": "a1uldvNpV_wv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install --disable-pip-version-check \\\n",
        "    torch==1.13.1 \\\n",
        "    torchdata==0.5.1 --quiet\n",
        "\n",
        "%pip install \\\n",
        "    transformers==4.27.2 \\\n",
        "    datasets==2.11.0  --quiet"
      ],
      "metadata": {
        "id": "CfaN2dRQWJDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import GenerationConfig"
      ],
      "metadata": {
        "id": "S1WbaWGfXL1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='2'></a>\n",
        "## 2 - Summarize Dialogue without Prompt Engineering\n",
        "\n",
        "In this use case, you will be generating a summary of a dialogue with the pre-trained Large Language Model (LLM) FLAN-T5 from Hugging Face. The list of available models in the Hugging Face `transformers` package can be found [here](https://huggingface.co/docs/transformers/index).\n",
        "\n",
        "Let's upload some simple dialogues from the [DialogSum](https://huggingface.co/datasets/knkarthick/dialogsum) Hugging Face dataset. This dataset contains 10,000+ dialogues with the corresponding manually labeled summaries and topics."
      ],
      "metadata": {
        "id": "dBPNepM_XUZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
        "\n",
        "dataset = load_dataset(huggingface_dataset_name)"
      ],
      "metadata": {
        "id": "FqnPkZJJXXJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices = [40, 200]\n",
        "\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "\n",
        "for i, index in enumerate(example_indices):\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print('INPUT DIALOGUE:')\n",
        "    print(dataset['test'][index]['dialogue'])\n",
        "    print(dash_line)\n",
        "    print('BASELINE HUMAN SUMMARY:')\n",
        "    print(dataset['test'][index]['summary'])\n",
        "    print(dash_line)\n",
        "    print()"
      ],
      "metadata": {
        "id": "GL1yqGgYXg14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the [FLAN-T5 model](https://huggingface.co/docs/transformers/model_doc/flan-t5), creating an instance of the `AutoModelForSeq2SeqLM` class with the `.from_pretrained()` method."
      ],
      "metadata": {
        "id": "TARY_Hj1XnDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name='google/flan-t5-base'\n",
        "model_name='google/flan-t5-xxl'\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "QSx3V6eVXpFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform encoding and decoding, you need to work with text in a tokenized form. **Tokenization** is the process of splitting texts into smaller units that can be processed by the LLM models.\n",
        "\n",
        "Download the tokenizer for the FLAN-T5 model using `AutoTokenizer.from_pretrained()` method. Parameter `use_fast` switches on fast tokenizer. At this stage, there is no need to go into the details of that, but you can find the tokenizer parameters in the [documentation](https://huggingface.co/docs/transformers/v4.28.1/en/model_doc/auto#transformers.AutoTokenizer)."
      ],
      "metadata": {
        "id": "IRlY-RWPXxyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "xpfo1xOTXzXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"What time is it, Tom?\"\n",
        "\n",
        "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "sentence_decoded = tokenizer.decode(\n",
        "        sentence_encoded[\"input_ids\"][0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print('ENCODED SENTENCE:')\n",
        "print(sentence_encoded[\"input_ids\"][0])\n",
        "print('\\nDECODED SENTENCE:')\n",
        "print(sentence_decoded)"
      ],
      "metadata": {
        "id": "K-q8JOeNX8Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    inputs = tokenizer(dialogue, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{dialogue}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - WITHOUT PROMPT ENGINEERING:\\n{output}\\n')"
      ],
      "metadata": {
        "id": "F7nw_9FiYFti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3'></a>\n",
        "## 3 - Summarize Dialogue with an Instruction Prompt\n",
        "\n",
        "Prompt engineering is an important concept in using foundation models for text generation. You can check out [this blog](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering) from Amazon Science for a quick introduction to prompt engineering."
      ],
      "metadata": {
        "id": "kMZkOx10YN0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Summarize the following conversation.\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "    \"\"\"\n",
        "\n",
        "    # Input constructed prompt instead of the dialogue.\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{prompt}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ],
      "metadata": {
        "id": "_ipMMjTtYPqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='3.2'></a>\n",
        "### 3.2 - Zero Shot Inference with the Prompt Template from FLAN-T5\n",
        "\n",
        "Let's use a slightly different prompt. FLAN-T5 has many prompt templates that are published for certain tasks [here](https://github.com/google-research/FLAN/tree/main/flan/v2). In the following code, you will use one of the [pre-built FLAN-T5 prompts](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py):"
      ],
      "metadata": {
        "id": "nKjW6_DvYfta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, index in enumerate(example_indices):\n",
        "    dialogue = dataset['test'][index]['dialogue']\n",
        "    summary = dataset['test'][index]['summary']\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "\"\"\"\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    print(dash_line)\n",
        "    print('Example ', i + 1)\n",
        "    print(dash_line)\n",
        "    print(f'INPUT PROMPT:\\n{prompt}')\n",
        "    print(dash_line)\n",
        "    print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "    print(dash_line)\n",
        "    print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ],
      "metadata": {
        "id": "tv_d1QnpYciK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4'></a>\n",
        "## 4 - Summarize Dialogue with One Shot and Few Shot Inference\n",
        "\n",
        "**One shot and few shot inference** are the practices of providing an LLM with either one or more full examples of prompt-response pairs that match your task - before your actual prompt that you want completed. This is called \"in-context learning\" and puts your model into a state that understands your specific task.  You can read more about it in [this blog from HuggingFace](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api)."
      ],
      "metadata": {
        "id": "HY-o0jO2YnfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prompt(example_indices_full, example_index_to_summarize):\n",
        "    prompt = ''\n",
        "    for index in example_indices_full:\n",
        "        dialogue = dataset['test'][index]['dialogue']\n",
        "        summary = dataset['test'][index]['summary']\n",
        "\n",
        "        # The stop sequence '{summary}\\n\\n\\n' is important for FLAN-T5. Other models may have their own preferred stop sequence.\n",
        "        prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "{summary}\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    dialogue = dataset['test'][example_index_to_summarize]['dialogue']\n",
        "\n",
        "    prompt += f\"\"\"\n",
        "Dialogue:\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "What was going on?\n",
        "\"\"\"\n",
        "\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "DHQEIcy0YpKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "one_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(one_shot_prompt)"
      ],
      "metadata": {
        "id": "gttT3rBuY5vr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ONE SHOT:\\n{output}')"
      ],
      "metadata": {
        "id": "TvG5NbnoY-V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name='4.2'></a>\n",
        "### 4.2 - Few Shot Inference\n",
        "\n",
        "Let's explore few shot inference by adding two more full dialogue-summary pairs to your prompt."
      ],
      "metadata": {
        "id": "M1c2QCxYZnYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_indices_full = [40, 80, 120]\n",
        "example_indices_full = [80, 120]\n",
        "example_index_to_summarize = 200\n",
        "\n",
        "few_shot_prompt = make_prompt(example_indices_full, example_index_to_summarize)\n",
        "\n",
        "print(few_shot_prompt)"
      ],
      "metadata": {
        "id": "D7KPdMHXZoyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = dataset['test'][example_index_to_summarize]['summary']\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        max_new_tokens=50,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')"
      ],
      "metadata": {
        "id": "FZ6fvVNJaEXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can change the configuration parameters of the `generate()` method to see a different output from the LLM. So far the only parameter that you have been setting was `max_new_tokens=50`, which defines the maximum number of tokens to generate. A full list of available parameters can be found in the [Hugging Face Generation documentation](https://huggingface.co/docs/transformers/v4.29.1/en/main_classes/text_generation#transformers.GenerationConfig).\n",
        "\n",
        "A convenient way of organizing the configuration parameters is to use `GenerationConfig` class."
      ],
      "metadata": {
        "id": "4WIkgm1Ja8m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise:**\n",
        "\n",
        "Change the configuration parameters to investigate their influence on the output.\n",
        "\n",
        "Putting the parameter `do_sample = True`, you activate various decoding strategies which influence the next token from the probability distribution over the entire vocabulary. You can then adjust the outputs changing `temperature` and other parameters (such as `top_k` and `top_p`).\n",
        "\n",
        "Uncomment the lines in the cell below and rerun the code. Try to analyze the results. You can read some comments below."
      ],
      "metadata": {
        "id": "rGuqx9bRbAmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generation_config = GenerationConfig(max_new_tokens=50)\n",
        "# generation_config = GenerationConfig(max_new_tokens=10)\n",
        "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.1)\n",
        "generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=0.5)\n",
        "# generation_config = GenerationConfig(max_new_tokens=50, do_sample=True, temperature=1.0)\n",
        "\n",
        "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "    model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        generation_config=generation_config,\n",
        "    )[0],\n",
        "    skip_special_tokens=True\n",
        ")\n",
        "\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - FEW SHOT:\\n{output}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')"
      ],
      "metadata": {
        "id": "Oyo6VLOfa912"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print (inputs)\n",
        "print(dataset)\n",
        "print (dataset['test']['summary'][0])"
      ],
      "metadata": {
        "id": "blE0i5_RdiFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n"
      ],
      "metadata": {
        "id": "oc_dq2Z3lq_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "dash_line = '-'.join('' for x in range(100))"
      ],
      "metadata": {
        "id": "6BMA9rlHm8yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Summarize the following Book.\n",
        "\n",
        "{text}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "# Input constructed prompt instead of the dialogue.\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ],
      "metadata": {
        "id": "SK3s3xDbltU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = f\"\"\"\n",
        "What more is there to say about Whitney Houston, whose brilliant career, drug-taking and premature death has been hashed over exhaustively on screen and in the tabloids?  \\\n",
        "I Wanna Dance with Somebody tackles that question by taking her story back to the music. It’s written by New Zealander Anthony McCarten, whose screenplay for Bohemian Rhapsody didn’t dilute the tragedies in Freddie Mercury’s short life while still managing to evoke the exhilaration to be had from Queen’s greatest concerts.  \\\n",
        "It took a delicate touch and other rock stars haven’t been as fortunate. While Rocketman was just as exuberant in doing justice to Elton John as a performer, the Aretha Franklin biopic Respect (2021) became more depressing as it went on. The United States vs. Billie Holiday, also released last year, was the same. It was powerful but it was no celebration. Then there was Baz Luhrmann’s Elvis, which was more glitz than heart, clouded by the decision to tell the tale from the venal viewpoint of Presley’s manager, Colonel Tom Parker.  \\\n",
        "Members of Houston’s family were interviewed at length for a documentary four years ago. But for this film, also produced with the family’s co-operation, director Kasi Lemmons made extensive use of the singer’s original recordings, including those laid down en route to the final versions. It allowed star Naomi Ackie to take Houston’s false starts, double takes and spontaneous comments as prompts in shaping her performance. Although she was filmed singing, her voice was replaced by Houston’s.  \\\n",
        "She does well, catching the contradictions in Houston’s character – the naivete, the fear of failure, the rebellious streak and its effect on her religious convictions. But physically, Ackie bears only a superficial resemblance to the singer. With Houston’s image still so fresh in the memory, this is a distraction.  \\\n",
        "The script is impressionistic in style, covering a lot of ground by being selective and using Houston’s songs to drive the narrative. Much is made of her relationship with the record company chief Clive Davis (one of the film’s producers), who discovered Houston and helped her choose songs for her albums. He’s played by a wryly urbane Stanley Tucci, giving us a character whose iron fist is well covered in velvet. Appalled when Houston takes up smoking, he looks sadly at her as she lights up, remarking that it’s like watching a Stradivarius being left out in the rain.  \\\n",
        "The film falls a little short of its claim to be warts-and-all, condensing the low points in Houston’s turbulent marriage to Bobby Brown (Ashton Sanders), but it doesn’t hold back in dealing with her father’s financial mismanagement. Nor does it underplay the importance of her enduring friendship with her former lover, Robyn Crawford – played with a lot of verve by Nafessa Williams.  \\\n",
        "And while it leaves her death off-screen, preferring to go out with the show-stopping medley she delivered at the 1994 American Music Awards, I’m not complaining. Sandra Hall\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Evaluate the sentiment of the following quote between \"```\" \\\n",
        "from negative 10 to positive 10 and output for each just the numeric value, \\\n",
        "do not include any additional text, just the numeric value:\n",
        "```{text}```\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KPotOTiOp_1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ],
      "metadata": {
        "id": "J6Z7LNPYqEMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.austinchronicle.com/events/film/2022-12-23/whitney-houston-i-wanna-dance-with-somebody/\n",
        "# ★½  (-4)\n",
        "text = f\"\"\"\n",
        "Whitney Houston was an icon, a legend of the music industry and a breathtaking pop idol who cranked out hit after hit, shattering records that previously the Beatles and Elvis held. Houston had enormous range and depth, yet the film about this larger-than-life superstar is somehow one of the flattest, cheapest, and most underwhelming music biopics to ever land on the silver screen.  \\\n",
        "Directed by Kasi Lemmons (Harriet, Eve’s Bayou), Whitney Houston: I Wanna Dance With Somebody somehow manages to make the life of Houston so painfully dull and thin. Just as the heat begins to rise, the film skips ahead, or around, orbiting the chance to dig deeper, to offer a more meaningful glimpse into the pop star’s history. Anthony McCarten’s script coasts, breezing through Houston’s life with a coolness that is anticlimactic. Father issues, her divorce, queer identity struggles, and addiction are danced around, never really creating a deep impact on an emotional level. When the bombastic moments like Houston’s historic performances in a newly post-apartheid South Africa arrive, they do so with a whimper, with tacky CGI stadiums that bring a tepid energy to the watered-down performances.  \\\n",
        "There’s no bite to Naomi Ackie’s performance, and perhaps that is because she’s not given anything worth chewing on. Even the clothes they drape her in feel cheap and poorly made. One of the film’s greatest injustices is that, after Ackie’s “big” performance of the iconic “I Have Nothing” medley Houston sang at the 1994 American Music Awards, the live broadcast is immediately shown in the credits, highlighting the stark difference in quality. The movie replaces her lush velvet caped black dress with a fast-fashion, wrinkled gown that’s bejeweled with clunky plastic gems. This fizzled ending echoes the sentiment of the entire film, a watered-down TV-soap approach to its dramatic subject.  \\\n",
        "Whitney Houston: I Wanna Dance With Somebody is like the SparkNotes of her life, a smattering of collected moments that feel hollow. The film retroactively makes Baz Luhrmann’s Elvis look like a masterpiece for actually trying to be bedazzling and insane, because Whitney Houston: I Wanna Dance With Somebody is so stale it might as well have been shoved directly onto a streaming platform to wither away forgotten – unlike Houston’s discography, which will be remembered for decades to come.\n",
        "\"\"\"\n",
        "prompt = f\"\"\"\n",
        "Evaluate the sentiment of the following quote between \"```\" \\\n",
        "from negative 10 to positive 10 and output for each just the numeric value, \\\n",
        "do not include any additional text, just the numeric value:\n",
        "```{text}```\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "output = tokenizer.decode(\n",
        "        model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            max_new_tokens=50,\n",
        "        )[0],\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}\\n')"
      ],
      "metadata": {
        "id": "YiGTTw_iqVzc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}